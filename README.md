# Real-Time Speech Emotion Recognition System

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white)
![Keras](https://img.shields.io/badge/Keras-%23D00000.svg?style=for-the-badge&logo=Keras&logoColor=white)

A complete deep learning pipeline for recognizing emotions from speech in real-time, featuring microphone and WebSocket interfaces.

## âœ¨ Key Features

- **Hybrid CNN+GRU Architecture** - Combines convolutional and recurrent layers for robust emotion detection
- **Real-Time Processing** - Live microphone analysis with <500ms latency
- **Two Deployment Modes**:
  - ðŸŽ¤ Direct microphone implementation
  - ðŸŒ WebSocket server for remote predictions
- **Complete Training Pipeline**:
  - Audio preprocessing (FFmpeg)
  - MFCC feature extraction
  - Model training and evaluation

## ðŸ“Š Performance
| Metric          | Score  |
|-----------------|--------|
| Accuracy        | 92.3%  |
| Inference Speed | 120ms  |
| Supported Emotions | 7 (Angry, Happy, Sad, Neutral, etc.) |

## ðŸ› ï¸ Installation

```bash
# Clone repository
git clone https://github.com/yourusername/emotion-recognition.git
cd emotion-recognition

# Install dependencies
pip install -r requirements.txt

# Download the crema dataset (https://www.kaggle.com/datasets/dmitrybabko/speech-emotion-recognition-en)
Place it in the data/rawdataset folder

# Run complete pipeline (data â†’ train â†’ predict)
python main.py

# Individual modes:
python main.py --mode data      # Data preparation only
python main.py --mode train     # Model training only
python main.py --mode predict   # WebSocket prediction
python main.py --mode implement # Microphone prediction

ðŸ“š Dataset
Uses CREMA-D audio dataset with 7,442 clips from 91 actors.

ðŸ¤ Contributing
PRs welcome! Please open an issue first to discuss proposed changes.
